{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaa43c4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import Model\n",
    "from keras.layers import Conv1D, Embedding, Input, Bidirectional, CuDNNLSTM, Dense, Concatenate, Masking, LSTM, SpatialDropout1D\n",
    "from keras.layers import BatchNormalization, Dropout, Activation\n",
    "from keras.layers import GlobalMaxPool1D, GlobalAveragePooling1D, GlobalAvgPool1D, GlobalMaxPooling1D\n",
    "from keras.layers import Subtract, Multiply\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, Callback\n",
    "from keras.utils import to_categorical\n",
    "from keras_radam import RAdam\n",
    "from keras_lookahead import Lookahead\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "876956b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    tf.compat.v1.set_random_seed(seed)\n",
    "\n",
    "seed = 2021\n",
    "fix_seed(seed)\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# print(gpus)\n",
    "# for gpu in gpus:\n",
    "#     tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89aa4438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39799, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('fact_checking_train.csv', sep='\\t')\n",
    "df_train['claim'] = df_train['author'] +' '+ df_train['claim'] \n",
    "df_test = pd.read_csv('fact_checking_test.csv', sep='\\t')\n",
    "df_test['claim'] = df_test['author'] +' '+ df_test['claim'] \n",
    "evidence = pd.read_csv('evidence.csv',sep='\\t')\n",
    "evidence.columns = ['ID','claim']\n",
    "evidence['author'] = 'NaN'\n",
    "evidence['label'] = -1\n",
    "label_2_v = {'pants-fire':0,'false':1,'barely-true':2,'half-true':3,'mostly-true':4,'true':5}\n",
    "df_train['label'] = df_train['label'].map(label_2_v)\n",
    "\n",
    "df_data = evidence.append(df_train)\n",
    "df_data = df_data.append(df_test)\n",
    "# df_data = df_train.append(df_test)\n",
    "df_data = df_data.reset_index(drop=True)\n",
    "df_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6419465a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>author</th>\n",
       "      <th>claim</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>Joe Biden Sanders’ “Medicare for All” plan \"wo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>Hillary Clinton McCain \"still thinks it's okay...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Facebook posts</td>\n",
       "      <td>Facebook posts Says a video shows Iranian rock...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Tom Barrett</td>\n",
       "      <td>Tom Barrett \"No one on my staff has ever been ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>City of Atlanta</td>\n",
       "      <td>City of Atlanta Tyler Perry’s plan to turn a m...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID           author                                              claim  \\\n",
       "0   0        Joe Biden  Joe Biden Sanders’ “Medicare for All” plan \"wo...   \n",
       "1   1  Hillary Clinton  Hillary Clinton McCain \"still thinks it's okay...   \n",
       "2   2   Facebook posts  Facebook posts Says a video shows Iranian rock...   \n",
       "3   3      Tom Barrett  Tom Barrett \"No one on my staff has ever been ...   \n",
       "4   4  City of Atlanta  City of Atlanta Tyler Perry’s plan to turn a m...   \n",
       "\n",
       "   label  \n",
       "0      1  \n",
       "1      1  \n",
       "2      1  \n",
       "3      1  \n",
       "4      3  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "939d8b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "I1 = np.load(\"I1.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f75cbe51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6576, 19510,  1769, 13226,  2348],\n",
       "       [19163, 15663,   747, 17304, 17501],\n",
       "       [17224, 10359, 12239,   368,  7150],\n",
       "       [  973, 12778, 13214,   545, 15582],\n",
       "       [18714,  8887, 16241, 11511, 13224]], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I1[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cd4e0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate seqs\n"
     ]
    }
   ],
   "source": [
    "max_words_num = None\n",
    "seq_len = 2000\n",
    "seq_len = 200\n",
    "embedding_dim = 32\n",
    "col = 'claim'\n",
    "\n",
    "print('Generate seqs')\n",
    "os.makedirs('seqs', exist_ok=True)\n",
    "seq_path = 'seqs/seqs_{}_{}.npy'.format(max_words_num, seq_len)\n",
    "word_index_path = 'seqs/word_index_{}_{}.npy'.format(max_words_num, seq_len)\n",
    "if not os.path.exists(seq_path) or not os.path.exists(word_index_path):\n",
    "    tokenizer = text.Tokenizer(num_words=max_words_num, lower=False, filters='')\n",
    "#     tokenizer.fit_on_texts(df_data[col].values.tolist())\n",
    "    tokenizer.fit_on_texts(df_train[col].values.tolist())\n",
    "    seqs = sequence.pad_sequences(tokenizer.texts_to_sequences(df_data[col].values.tolist()), maxlen=seq_len,\n",
    "                                  padding='post', truncating='pre')\n",
    "    word_index = tokenizer.word_index\n",
    "        \n",
    "    np.save(seq_path, seqs)\n",
    "    np.save(word_index_path, word_index)\n",
    "\n",
    "else:\n",
    "    seqs = np.load(seq_path)\n",
    "    word_index = np.load(word_index_path, allow_pickle=True).item()\n",
    "\n",
    "embedding = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "\n",
    "\n",
    "env = seqs[0:20006].copy()\n",
    "seqs = seqs[20006:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae3c9a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43021, 32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1807608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0    20006\n",
       " 1.0     4462\n",
       " 3.0     3171\n",
       " 2.0     2980\n",
       " 4.0     2898\n",
       " 5.0     2256\n",
       " 0.0     2233\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data['label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d72cb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('model', exist_ok=True)\n",
    "os.makedirs('sub', exist_ok=True)\n",
    "os.makedirs('prob', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f26d6a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_index = [i for i in range(18000)]\n",
    "# test_index = df_data[df_data['label'].isnull()].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccec4b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(emb, seq_len):\n",
    "    emb_layer = Embedding(\n",
    "        input_dim=emb.shape[0],\n",
    "        output_dim=emb.shape[1],\n",
    "        input_length=seq_len,\n",
    "    )\n",
    "    \n",
    "    seq = Input(shape=(seq_len, ))\n",
    "    seq_emb = emb_layer(seq)\n",
    "    \n",
    "    seq_emb = SpatialDropout1D(rate=0.5)(seq_emb)\n",
    "\n",
    "    lstm = Bidirectional(CuDNNLSTM(50, return_sequences=True))(seq_emb)\n",
    "#     lstm = Bidirectional(LSTM(200, return_sequences=True))(seq_emb)\n",
    "    \n",
    "    \n",
    "    lstm_avg_pool = GlobalAveragePooling1D()(lstm)\n",
    "    lstm_max_pool = GlobalMaxPooling1D()(lstm)\n",
    "    x = Concatenate()([lstm_avg_pool, lstm_max_pool])\n",
    "    \n",
    "    x = Dropout(0.5)(Activation(activation='relu')(BatchNormalization()(Dense(128)(x))))\n",
    "    out = Dense(6, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=seq, outputs=out)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=Lookahead(RAdam()), metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_model_multi_input(emb, seq_len):\n",
    "    emb_layer = Embedding(\n",
    "        input_dim=emb.shape[0],\n",
    "        output_dim=emb.shape[1],\n",
    "        input_length=seq_len,\n",
    "    )\n",
    "    \n",
    "    seq1 = Input(shape=(seq_len, ))\n",
    "    seq2 = Input(shape=(seq_len, ))\n",
    "    seq_emb1 = emb_layer(seq1)\n",
    "    seq_emb2 = emb_layer(seq2)\n",
    "    \n",
    "    shared_lstm = Bidirectional(CuDNNLSTM(50, return_sequences=True))\n",
    "    \n",
    "    seq_emb1 = SpatialDropout1D(rate=0.5)(seq_emb1)\n",
    "    seq_emb2 = SpatialDropout1D(rate=0.5)(seq_emb2)\n",
    "    \n",
    "    lstm1 = shared_lstm(seq_emb1)\n",
    "    lstm2 = shared_lstm(seq_emb2)    \n",
    "    \n",
    "    lstm_avg_pool1 = GlobalAveragePooling1D()(lstm1)\n",
    "    lstm_max_pool1 = GlobalMaxPooling1D()(lstm1)\n",
    "    lstm_avg_pool2 = GlobalAveragePooling1D()(lstm2)\n",
    "    lstm_max_pool2 = GlobalMaxPooling1D()(lstm2)\n",
    "    lstm_multiply_1 = Multiply()([lstm_avg_pool1,lstm_avg_pool2])\n",
    "    lstm_multiply_2 = Multiply()([lstm_max_pool1,lstm_max_pool2])\n",
    "    lstm_subtract_1 = Subtract()([lstm_avg_pool1,lstm_avg_pool2])\n",
    "    lstm_subtract_2 = Subtract()([lstm_max_pool1,lstm_max_pool2])\n",
    "    \n",
    "    \n",
    "    x = Concatenate()([lstm_avg_pool1, lstm_max_pool1, lstm_avg_pool2, lstm_max_pool2,\n",
    "                      lstm_multiply_1, lstm_multiply_2, lstm_subtract_1, lstm_subtract_2])\n",
    "    \n",
    "    x = Dropout(0.5)(Activation(activation='relu')(BatchNormalization()(Dense(128)(x))))\n",
    "    out = Dense(6, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=[seq1, seq2], outputs=out)\n",
    "\n",
    "#     sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(lr=0.0001), metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62f0db26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mh_chen\\anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 200, 32)      1376672     input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 200, 32)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_2 (SpatialDro (None, 200, 32)      0           embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 200, 100)     33600       spatial_dropout1d_1[0][0]        \n",
      "                                                                 spatial_dropout1d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 100)          0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 100)          0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 100)          0           bidirectional_1[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 100)          0           bidirectional_1[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 100)          0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 100)          0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "subtract_1 (Subtract)           (None, 100)          0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "subtract_2 (Subtract)           (None, 100)          0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 800)          0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 multiply_1[0][0]                 \n",
      "                                                                 multiply_2[0][0]                 \n",
      "                                                                 subtract_1[0][0]                 \n",
      "                                                                 subtract_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          102528      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128)          512         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128)          0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 6)            774         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,514,086\n",
      "Trainable params: 1,513,830\n",
      "Non-trainable params: 256\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = build_model_multi_input(embedding, seq_len)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d1e64e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39904488",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator(Callback):\n",
    "    def __init__(self, validation_data):\n",
    "        super().__init__()\n",
    "        self.best_val_f1 = 0.\n",
    "        self.x_val = validation_data[0]\n",
    "        self.y_val = validation_data[1]\n",
    "\n",
    "    def evaluate(self):\n",
    "        y_true = self.y_val\n",
    "        y_pred = self.model.predict(self.x_val).argmax(axis=1)\n",
    "        f1 = f1_score(y_true, y_pred, average='macro')\n",
    "        return f1\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_f1 = self.evaluate()\n",
    "        if val_f1 > self.best_val_f1:\n",
    "            self.best_val_f1 = val_f1\n",
    "        logs['val_f1'] = val_f1\n",
    "        print(f'val_f1: {val_f1:.5f}, best_val_f1: {self.best_val_f1:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b9ee383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mh_chen\\anaconda3\\envs\\tf1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 16200 samples, validate on 1800 samples\n",
      "Epoch 1/30\n",
      "16200/16200 [==============================] - 462s 28ms/step - loss: 1.7529 - accuracy: 0.2296 - val_loss: 1.6882 - val_accuracy: 0.2667\n",
      "val_f1: 0.23529, best_val_f1: 0.23529\n",
      "\n",
      "Epoch 00001: val_f1 improved from -inf to 0.23529, saving model to model/lstm_0.h5\n",
      "Epoch 2/30\n",
      "16200/16200 [==============================] - 201s 12ms/step - loss: 1.6811 - accuracy: 0.2588 - val_loss: 1.6643 - val_accuracy: 0.2678\n",
      "val_f1: 0.17754, best_val_f1: 0.23529\n",
      "\n",
      "Epoch 00002: val_f1 did not improve from 0.23529\n",
      "Epoch 3/30\n",
      "16200/16200 [==============================] - 177s 11ms/step - loss: 1.6460 - accuracy: 0.2865 - val_loss: 1.6391 - val_accuracy: 0.3122\n",
      "val_f1: 0.24795, best_val_f1: 0.24795\n",
      "\n",
      "Epoch 00003: val_f1 improved from 0.23529 to 0.24795, saving model to model/lstm_0.h5\n",
      "Epoch 4/30\n",
      "16200/16200 [==============================] - 176s 11ms/step - loss: 1.6051 - accuracy: 0.3100 - val_loss: 1.6393 - val_accuracy: 0.3094\n",
      "val_f1: 0.24433, best_val_f1: 0.24795\n",
      "\n",
      "Epoch 00004: val_f1 did not improve from 0.24795\n",
      "Epoch 5/30\n",
      "16200/16200 [==============================] - 173s 11ms/step - loss: 1.5859 - accuracy: 0.3267 - val_loss: 1.6558 - val_accuracy: 0.2917\n",
      "val_f1: 0.18762, best_val_f1: 0.24795\n",
      "\n",
      "Epoch 00005: val_f1 did not improve from 0.24795\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Epoch 6/30\n",
      "16200/16200 [==============================] - 174s 11ms/step - loss: 1.5495 - accuracy: 0.3485 - val_loss: 1.6297 - val_accuracy: 0.3261\n",
      "val_f1: 0.31163, best_val_f1: 0.31163\n",
      "\n",
      "Epoch 00006: val_f1 improved from 0.24795 to 0.31163, saving model to model/lstm_0.h5\n",
      "Epoch 7/30\n",
      "16200/16200 [==============================] - 173s 11ms/step - loss: 1.5225 - accuracy: 0.3674 - val_loss: 1.6257 - val_accuracy: 0.3144\n",
      "val_f1: 0.28337, best_val_f1: 0.31163\n",
      "\n",
      "Epoch 00007: val_f1 did not improve from 0.31163\n",
      "Epoch 8/30\n",
      " 7542/16200 [============>.................] - ETA: 1:29 - loss: 1.5052 - accuracy: 0.3727"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 32, 50, 1, 200, 2, 50] \n\t [[{{node training/Adam/gradients/bidirectional_2_1/CudnnRNN_1_grad/CudnnRNNBackprop}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7868/1943092864.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m     model.fit([train_x,train_env], train_y, batch_size=bs, epochs=30,\n\u001b[0;32m     28\u001b[0m               \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_env\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m               callbacks=[Evaluator(validation_data=([val_x,val_env], val_y)), checkpoint, reduce_lr, earlystopping], verbose=1, shuffle=True)\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf1\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf1\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3476\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1472\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1473\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 32, 50, 1, 200, 2, 50] \n\t [[{{node training/Adam/gradients/bidirectional_2_1/CudnnRNN_1_grad/CudnnRNNBackprop}}]]"
     ]
    }
   ],
   "source": [
    "bs = 2\n",
    "monitor = 'val_f1'\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "for fold_id, (train_index, val_index) in enumerate(kfold.split(all_index, df_train['label'])):\n",
    "    train_x = seqs[train_index]\n",
    "    val_x = seqs[val_index]\n",
    "    \n",
    "    train_env = env[I1[train_index][:,0]]\n",
    "    val_env = env[I1[val_index][:,0]]\n",
    "\n",
    "    label = df_train['label'].values\n",
    "    train_y = label[train_index]\n",
    "    val_y = label[val_index]\n",
    "    \n",
    "    model_path = 'model/lstm_{}.h5'.format(fold_id)\n",
    "    checkpoint = ModelCheckpoint(model_path, monitor=monitor, verbose=1, save_best_only=True, mode='max', save_weights_only=True)\n",
    "    earlystopping = EarlyStopping(monitor=monitor, patience=5, verbose=1, mode='max')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor=monitor, factor=0.5, patience=2, mode='max', verbose=1)\n",
    "    \n",
    "#     model = build_model(embedding, seq_len)\n",
    "#     model.fit(train_x, train_y, batch_size=bs, epochs=30,\n",
    "#               validation_data=(val_x, val_y),\n",
    "#               callbacks=[Evaluator(validation_data=(val_x, val_y)), checkpoint, reduce_lr, earlystopping], verbose=1, shuffle=True)\n",
    "    \n",
    "    model = build_model_multi_input(embedding, seq_len)\n",
    "    model.fit([train_x,train_env], train_y, batch_size=bs, epochs=30,\n",
    "              validation_data=([val_x,val_env], val_y),\n",
    "              callbacks=[Evaluator(validation_data=([val_x,val_env], val_y)), checkpoint, reduce_lr, earlystopping], verbose=1, shuffle=True)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e64783",
   "metadata": {},
   "outputs": [],
   "source": [
    "env[I1[train_index][:,0]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5551afa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
